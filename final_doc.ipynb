{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd18539",
   "metadata": {},
   "source": [
    "###### About the dataset\n",
    "The dataset chosen contains data about Airline Travel. The dataset has the intents and user raw input conversations which contain the queries about the flights availability, flight fares, details about airlines, flight timings in it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7837e48",
   "metadata": {},
   "source": [
    "#### Web Service Model for the Chatbot\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb9f94",
   "metadata": {},
   "source": [
    "To host our NLP model for Intent classification, Name Entity recognition and dialogue flow manager, Flask API framework is choosen. Flask is python web development framework with an inbuilt web server, using which APIs are developed using different machine learning python libraries such as tensorflow, nltk, sklearn, keras, spacy etc. Flask is chosen as our serving model not only because it is easy to integrate with web pages and takes little effort to make a application up and running with API end points but also it is comparatively flexible and readable than anyother API deployment models. We did explore other  machine learning deployment model service such as FastAPI, Seldon Core, DeepDetect etc in which few of them focus only on serving one component and few are complex to understand and implement. FastAPI is known for its performance, but is relatively new, not many resources were available for achieving our goal. \n",
    "\n",
    "As our goal was to make a custom UI to serve our chatbot model, Flask was the right choice as it gives more flexibility to work with html pages and smooth dataflow between UI and backend. The application deployment is also easy and fast with Flask API. \n",
    "\n",
    "As we are going with docker deployment, base images for building a flask application(eg.python:3.8-slim-buster) are readily available in docker hub and installing the python libraries and making the application run as a container is quite easily acheviable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5ad095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intents</th>\n",
       "      <th>Input_Queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what's the arrival time in sanfrancisco for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Intents                                      Input_Queries\n",
       "0       atis_flight   i want to fly from boston at 838 am and arriv...\n",
       "1       atis_flight   what flights are available from pittsburgh to...\n",
       "2  atis_flight_time   what's the arrival time in sanfrancisco for t...\n",
       "3      atis_airfare            cheapest airfare from tacoma to orlando\n",
       "4      atis_airfare   round trip fares from pittsburgh to philadelp..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flight_details = pd.read_csv('atis_data.csv')\n",
    "\n",
    "df = pd.DataFrame(flight_details)\n",
    "\n",
    "\n",
    "flight_details.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2315bf46",
   "metadata": {},
   "source": [
    "#### Logging and monitoring\n",
    "We are using python logging library to track the events that happen in the application. Logging library helps to capture the log information with the level of severity - debug, info, warning, error and critical. \n",
    "\n",
    "Using format as current date time and display the log message beside the datetime so that the loggers are captured with the exact time when the event happened\n",
    "\n",
    "We are saving the log information into a file 'atis_details_bot.log' and maintaining in the same directory in which the code resides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4980ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='atis_ChatBot.log', encoding='utf-8', level=logging.DEBUG, format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024c6c9",
   "metadata": {},
   "source": [
    "### NER implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a2e0a",
   "metadata": {},
   "source": [
    "To get the name entities for the given query, we initially used Spacy pretrained pipeline. It has some limitations while extracting few entities for this dataset,like GPE is specified for both the source and destination locations which doesn't give the exact information about whether the location is source or destination.Hence, we used Spacy rule based entity recognizer to extract the exact source and destination locations.\n",
    "\n",
    "Then the obtained entities are trained with Spacy custom NER with 70-30 train-test split and 50-50 train-test split and checked the metrics like accuracy, f1 score, recall and precision which was nearly 100% \n",
    "\n",
    "Using this approach, we retrived the both default entity labels using spacy pretrained pipeline, and source and destination entities from the given text. These entities can be combined to be used in the conversations to get the output for the given query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50784c26",
   "metadata": {},
   "source": [
    "#### Data preprocessing for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d019c8a",
   "metadata": {},
   "source": [
    "The data is already clean and doesnt contain nulls. Also, stop words removal cannot be done since the needed information for the rule based entity recognition('from','in','to' etc) will be removed\n",
    "\n",
    "Hence, applying basic preprocessing like removing the punctuations, making the words to lower case and performing Lemmatization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e72ad8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Dharani\n",
      "[nltk_data]     Rayadurgam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "nlppipeline = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner = nlppipeline.get_pipe(\"ner\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "optimizer = nlp.resume_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "983f0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    logging.info('Preprocessing for NER')\n",
    "    input_convos = df['Input_Queries']\n",
    "\n",
    "    # Removing punctuation\n",
    "    df['final_processed_data'] = input_convos.map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "    # Converting the dataset to lowercase\n",
    "    df['final_processed_data'] = input_convos.map(lambda x: x.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_processed_data = []\n",
    "    for data in df['final_processed_data']:\n",
    "        lemmatized_data = lemmatizer.lemmatize(data)\n",
    "        final_processed_data.append(lemmatized_data)\n",
    "\n",
    "    df['final_processed_data'] = final_processed_data\n",
    "    #df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb1563b",
   "metadata": {},
   "source": [
    "##### NER using spacy pipeline\n",
    "Getting the Name entities using the spacy pipeline 'en_core_web_sm' for entity labels other than GPE(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24b4ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_ner(df):\n",
    "    logging.info('Getting NER using spacy pipeline')\n",
    "    query = nlp(df)\n",
    "    ners = {}\n",
    "    for word in query.ents:\n",
    "        if word.label_ != 'GPE':\n",
    "            ners[word.label_] = word.text\n",
    "    logging.info('Obtained NERs from spacy pipeline', ners)\n",
    "    return ners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197ebc2",
   "metadata": {},
   "source": [
    "##### Model 1  Rule based entity recognizer using spacy\n",
    "Using these rules to distingish the source and destination location and adding them to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a67faf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_based_ner(df):\n",
    "    logging.debug('Entering NER rule based training ')\n",
    "    data = df['final_processed_data']\n",
    "    sourceloc = ''\n",
    "    destloc = ''\n",
    "    destination = []\n",
    "    source = []\n",
    "    source_data = []\n",
    "    destination_data = []\n",
    "    for doc in data:\n",
    "        words_list = doc.split()\n",
    "\n",
    "        if ' from ' in doc:\n",
    "            sourceloc = words_list[words_list.index('from') + 1]\n",
    "        elif ' leaving ' in doc:\n",
    "            sourceloc = words_list[words_list.index('leaving') + 1]\n",
    "        else:\n",
    "            doc = doc + ' na'\n",
    "            sourceloc = 'na'\n",
    "        nlpner = English()\n",
    "        ruler = nlpner.add_pipe(\"entity_ruler\")\n",
    "        source_rules = [{\"label\": \"source\", \"pattern\": [{\"LOWER\": sourceloc}]}]\n",
    "        sourceloc = ''\n",
    "        ruler.add_patterns(source_rules)\n",
    "\n",
    "        if ' in ' in doc:\n",
    "            destloc = words_list[words_list.index('in') + 1]\n",
    "        elif ' to ' in doc:\n",
    "            destloc = words_list[words_list.index('to') + 1]\n",
    "        else:\n",
    "            destloc = 'na'\n",
    "            doc = doc + ' na'\n",
    "        dest_rules = [{\"label\": \"destination\", \"pattern\": [{\"LOWER\": destloc}]}]\n",
    "        ruler.add_patterns(dest_rules)\n",
    "        destloc = ''\n",
    "        sourceloc = ''\n",
    "        doc1 = nlpner(doc)\n",
    "        for entity in doc1.ents:\n",
    "            if entity.label_ == 'source':\n",
    "                source.append(entity.text)\n",
    "                source_data.append((doc, {'entities': [(doc.index(entity.text),\n",
    "                                                        doc.index(entity.text) + len(entity.text),\n",
    "                                                        'SOURCE_LOC')]}))\n",
    "                break\n",
    "        for entity in doc1.ents:\n",
    "            if entity.label_ == 'destination':\n",
    "                destination.append(entity.text)\n",
    "                destination_data.append((doc, {'entities': [(doc.index(entity.text),\n",
    "                                                             doc.index(entity.text) + len(\n",
    "                                                                 entity.text),\n",
    "                                                             'DESTINATION_LOC')]}))\n",
    "                break\n",
    "\n",
    "\n",
    "    df['source'] = source\n",
    "    df['destination'] = destination\n",
    "    logging.debug('Completed NER rule based entity recognition')\n",
    "    return source_data,destination_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff14735",
   "metadata": {},
   "source": [
    "##### Inference from the above output\n",
    "By executing the above methods, the source and destination of the input query is properly distingused and provided with the rule defined labels - source and destination  and the data required for custom NER for source and destination is obtained\n",
    "\n",
    "Printing the dataset with the source and destination appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d107769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intents</th>\n",
       "      <th>Input_Queries</th>\n",
       "      <th>final_processed_data</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "      <td>boston</td>\n",
       "      <td>denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "      <td>pittsburgh</td>\n",
       "      <td>baltimore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what's the arrival time in sanfrancisco for t...</td>\n",
       "      <td>what's the arrival time in sanfrancisco for t...</td>\n",
       "      <td>washington</td>\n",
       "      <td>sanfrancisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "      <td>tacoma</td>\n",
       "      <td>orlando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "      <td>pittsburgh</td>\n",
       "      <td>philadelphia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Intents                                      Input_Queries  \\\n",
       "0       atis_flight   i want to fly from boston at 838 am and arriv...   \n",
       "1       atis_flight   what flights are available from pittsburgh to...   \n",
       "2  atis_flight_time   what's the arrival time in sanfrancisco for t...   \n",
       "3      atis_airfare            cheapest airfare from tacoma to orlando   \n",
       "4      atis_airfare   round trip fares from pittsburgh to philadelp...   \n",
       "\n",
       "                                final_processed_data      source   destination  \n",
       "0   i want to fly from boston at 838 am and arriv...      boston        denver  \n",
       "1   what flights are available from pittsburgh to...  pittsburgh     baltimore  \n",
       "2   what's the arrival time in sanfrancisco for t...  washington  sanfrancisco  \n",
       "3            cheapest airfare from tacoma to orlando      tacoma       orlando  \n",
       "4   round trip fares from pittsburgh to philadelp...  pittsburgh  philadelphia  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info('Performing NER preprocessing and rule based')\n",
    "data_preprocessing(df)\n",
    "source_data, destination_data = rule_based_ner(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8d602",
   "metadata": {},
   "source": [
    "##### Model 2 Custom NER modelling\n",
    "Using the labels derived above to train the data using spacy custom NER minibatch approach and fetching the loss for 30 iterations\n",
    "Stored the data as required to train with spacy custom NER in source_train_data with custom entity 'SOURCE_LOC' and destination_train_data with custom entity 'DESTINATION_LOC'\n",
    "Everytime, the data is shuffled and trained in batches for 30 iterations so that the model dont remember the labels as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f47a965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlppipeline=spacy.load('en_core_web_sm')\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner=nlppipeline.get_pipe(\"ner\")\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0e5a550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ner_train_test_split(source_data,destination_data):\n",
    "    logging.info('Splitting the data to train and test')\n",
    "    n = len(source_data)\n",
    "    print('Total data length: ', n)\n",
    "    train_data_size = n * 0.7\n",
    "    test_data_size = n * 0.3\n",
    "    source_train_data = source_data[0:int(train_data_size)]\n",
    "    source_test_data = source_data[int(train_data_size):]\n",
    "    destination_train_data = destination_data[0:int(train_data_size)]\n",
    "    destination_test_data = destination_data[int(train_data_size):]\n",
    "    print('source split: ', len(source_train_data), len(source_test_data))\n",
    "    print('destination split: ', len(destination_train_data), len(destination_test_data))\n",
    "    return source_train_data,source_test_data,destination_train_data,destination_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c62aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_modelling(label, traindata):\n",
    "    logging.debug('Entering NER custom training model')\n",
    "\n",
    "    output_path = ''\n",
    "    for _, annotates in traindata:\n",
    "        for ent in annotates.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    # Disabling the components otherthan the required ones\n",
    "    unaffected_pipelines = [pipeline for pipeline in nlppipeline.pipe_names if\n",
    "                            pipeline not in [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]]\n",
    "\n",
    "    # Model Training with 40 iterations so that it wont remember the data\n",
    "    with nlppipeline.disable_pipes(*unaffected_pipelines):\n",
    "\n",
    "        for iteration in range(30):\n",
    "            random.shuffle(traindata)\n",
    "            losses = {}\n",
    "            #  using spaCy's minibatch to batch up the train data\n",
    "            allbatches = minibatch(traindata, size=compounding(5.0, 30.0, 1.001))\n",
    "            for eachbatch in allbatches:\n",
    "                for txt, annotates in eachbatch:\n",
    "                    doc = nlppipeline.make_doc(txt)\n",
    "                    example = Example.from_dict(doc, annotates)\n",
    "                    # Running nlppipeline.update to adjust the weights\n",
    "                    nlppipeline.update([example], losses=losses, drop=0.3)\n",
    "                    # print(losses)\n",
    "\n",
    "    # Saving the model to path same as the label so that it can be loaded from the same path again\n",
    "\n",
    "    output_path = Path(label)\n",
    "    logging.info(\"Saving the model to\", output_path)\n",
    "    nlppipeline.to_disk(output_path)\n",
    "    logging.debug('Leaving the NER custome model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cf25e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_spacy_ner(source_train_data,destination_train_data):\n",
    "    logging.info('Training NER custom model for source entity')\n",
    "    custom_modelling('source', source_train_data)\n",
    "    logging.info('Training NER custom model for destination entity')\n",
    "    custom_modelling('destination', destination_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a5a0728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data length:  199\n",
      "source split:  139 60\n",
      "destination split:  139 60\n"
     ]
    }
   ],
   "source": [
    "source_train_data, source_test_data, destination_train_data, destination_test_data = ner_train_test_split(source_data, destination_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "45826439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\logging\\__init__.py\", line 1083, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\logging\\__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\logging\\__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\logging\\__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/984242196.py\", line 1, in <module>\n",
      "    custom_spacy_ner(source_train_data, destination_train_data)\n",
      "  File \"C:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/3704397227.py\", line 3, in custom_spacy_ner\n",
      "    custom_modelling('source', source_train_data)\n",
      "  File \"C:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/2531400058.py\", line 31, in custom_modelling\n",
      "    logging.info(\"Saving the model to\", output_path)\n",
      "Message: 'Saving the model to'\n",
      "Arguments: (WindowsPath('source'),)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/984242196.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcustom_spacy_ner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_train_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/3704397227.py\u001b[0m in \u001b[0;36mcustom_spacy_ner\u001b[1;34m(source_train_data, destination_train_data)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcustom_modelling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'source'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training NER custom model for destination entity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mcustom_modelling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'destination'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/2531400058.py\u001b[0m in \u001b[0;36mcustom_modelling\u001b[1;34m(label, traindata)\u001b[0m\n\u001b[0;32m     23\u001b[0m                     \u001b[0mexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                     \u001b[1;31m# Running nlppipeline.update to adjust the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                     \u001b[0mnlppipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                     \u001b[1;31m# print(losses)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;31m# ignore statements are used here because mypy ignores hasattr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"update\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msgd\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOutT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\ml\\tb_framework.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     step_model = ParserStepModel(\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\ml\\parser_model.pyx\u001b[0m in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\with_array.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSeqT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeqT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSeqT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         return _ragged_forward(\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRagged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRagged\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRagged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\with_array.py\u001b[0m in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     88\u001b[0m ) -> Tuple[Ragged, Callable]:\n\u001b[0;32m     89\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mArrayXd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mArrayXd\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_dX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataXd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdYr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRagged\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mRagged\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\concatenate.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\concatenate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\hashembed.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, ids, is_train)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mseed\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"seed\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0mdrop_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "custom_spacy_ner(source_train_data, destination_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa4fbc",
   "metadata": {},
   "source": [
    "#### NER inference\n",
    "Using Spacy NER pipeline, rule based NER recognistion and custom NER modelling, we were able to obtain the required name entities for the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aef1bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner(query):\n",
    "    logging.debug('Entering get_ner')\n",
    "    source = ''\n",
    "    dest = ''\n",
    "    all_entities=nlp_ner(query)\n",
    "    for output_dir in ['source','destination']:\n",
    "        logging.info(\"Loading from\", output_dir)\n",
    "        move_names = list(ner.move_names)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        #assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        logging.info(query)\n",
    "        doc2 = nlp2(query)\n",
    "\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "            if ent.label_=='SOURCE_LOC':\n",
    "                obtained_source=ent.text\n",
    "                source= obtained_source\n",
    "            if ent.label_=='DESTINATION_LOC':\n",
    "                obtained_dest=ent.text\n",
    "                dest=obtained_dest\n",
    "\n",
    "    location_entities={'source':source,'dest':dest}\n",
    "    result=all_entities | location_entities\n",
    "    logging.info(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630edde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11c07b72",
   "metadata": {},
   "source": [
    "### Using Heuristics for Dialogue flow Manager\n",
    "\n",
    "For Dialogue Flow, We used multiple models such as RNN(Recurrent Neural Network), LSTM(Long-Short Term Memory) and did encoding/decoding too for the dialogue flow management. For our dataset, we don't have responses for the questions being asked. Using models such as RNN,LSTM, was difficult to generate text from these models. And without proper pre-defined responses and enough data, it was difficult to train the RNN/LSTM models. \n",
    "\n",
    "Hence we decided to proceed with rule-based/Heuristics approach for Dialogue Manager. This method gets the intent and Entities for the given query, and using the heuristics, generates response easily. \n",
    "\n",
    "Here, since we dont have the output of the input queries in our dataset, we defined a format in the reply message based on the intents and entities and displaying the other values such as flight names, flight fares, air craft prices as a random value. \n",
    "\n",
    "Usually, the exact details of the flights names, timings, fares etc could be maintained in a database and using our intents and NERs, the exact value of each could be fetched using the queries and replaced in the random values that we display\n",
    "\n",
    "As it is not a model, it doesn't keep any history of previous conversations yet. We could however make a mechanism to keep history of the conversation and also check if the user directs the question to some other topic other than what the bot is trained for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4207a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc25e8b2",
   "metadata": {},
   "source": [
    "#### Including greetings and farewell in the dialogue flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2b4d7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from flask import Flask,request,jsonify\n",
    "from werkzeug.wrappers import Request, Response\n",
    "from werkzeug.serving import run_simple\n",
    "\n",
    "app = Flask(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "204f2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "greetings= [\"hi\", \"hello\", \"hey\", \"helloo\", \"hellooo\", \"g morining\", \"gmorning\", \"good morning\",\n",
    "            \"morning\", \"good day\", \"good afternoon\", \"good evening\", \"greetings\",\n",
    "            \"greeting\", \"good to see you\", \"its good seeing you\", \"how are you\",\n",
    "            \"how're you\", \"how are you doing\", \"how ya doin'\", \"how ya doin\",\n",
    "             \"how is you\", \"how's you\", \"how is it going\", \"how's it going\", \"how's it goin'\",\n",
    "            \"how's it goin\", \"what is up\",  \"gday\", \"howdy\"]\n",
    "farewell=[\"Thank you\",\"Bye\", \"Good day\",\"good bye\",\"thanks\",\"have a good day\",\"cheers\",\"thank you so much for your help\",\"see you\",\n",
    "         \"bye-bye\", \"have a good day\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "662f64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data = {\"text\": \"Hi\",\n",
    "    \"Intents\": \"atis_flight\",\n",
    "        \"Entities\": {\"source\":\"buffalo\", \"dest\":\"oakland\"}}\n",
    "logging.info('Collecting data to display in the bot output')\n",
    "\n",
    "\n",
    "flights = ['Panam Airlines', 'American Airlines', 'Virgin Airlines', 'United Airlines', \n",
    "                'Breeze Airlines','Alaska Airlines','Frontier Airlines', 'JetBlue']\n",
    "\n",
    "aircraft_types = ['Airbus A320 family', 'Boeing 737 NG', 'Boeing 777','Airbus A330','Boeing 747','Airbus A319']\n",
    "\n",
    "flight_time = list(range(1,13))\n",
    "\n",
    "flight_fares = list(range(100,1000,50))\n",
    "\n",
    "\n",
    "    \n",
    "@app.route('/chat',methods=['POST'])\n",
    "def get_dialogue():\n",
    "    if request.method == 'POST':\n",
    "        input_query = request.form.get('msg')\n",
    "    log.debug('Entering dialogue flow manager')\n",
    "    log.info('Checking greetings and farewell intents')\n",
    "    if input_query.lower() in greetings:\n",
    "        intents='greetings'\n",
    "        entities={}\n",
    "    elif input_query.lower() in farewell:\n",
    "        intents='farewell'\n",
    "        entities={}\n",
    "    else:\n",
    "        log.info('Checking other intents by calling get_intents and fetching the NERs from get_ner method')\n",
    "        \n",
    "        intents = 'atis_flight'#get_intents(input_query)\n",
    "    \n",
    "        entities= get_ner(input_query)\n",
    "    \n",
    "    if len(entities)!= 0:\n",
    "        if 'source' in entities.keys():\n",
    "            source = entities['source']\n",
    "        \n",
    "        if 'dest' in entities.keys():\n",
    "            dest = entities['dest']\n",
    "    if intents == 'greetings':\n",
    "        output='Hello! Nice meeting you! How can I help'\n",
    "    elif intents== 'farewell':\n",
    "        output='Thank you for using ATIS chat bot! Have a good day'\n",
    "    \n",
    "    elif intents == 'atis_flight':\n",
    "        if source!= None and dest!= None:\n",
    "            random_flights = random.sample(flights, 3)\n",
    "            random_time = random.sample(flight_time, 3)\n",
    "            output = \"Below are the flights from \"+source+\" to \"+dest+\" :\"+random_flights[0]+\" at \"+str(random_time[0])+\"PM, \"+random_flights[1]+\" at \"+str(random_time[1])+\"PM, \"+random_flights[2]+\" at \"+str(random_time[2])+\"PM.\"\n",
    "        \n",
    "        elif source == None or dest == None:\n",
    "            output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "\n",
    "    elif intents == 'atis_flight_time':\n",
    "        if source!= None and dest!= None:\n",
    "            x = \"Here are the timings and flights available for today \"\n",
    "            random_flights = random.sample(flights, 3)\n",
    "            random_time = random.sample(flight_time, 3)\n",
    "            random_time.sort()\n",
    "            output = x+\"from \"+source+\" to \"+dest+\": \"+random_flights[0]+\" at \"+str(random_time[0])+\"PM, \"+random_flights[1]+\" at \"+str(random_time[1])+\"PM, \"+random_flights[2]+\" at \"+str(random_time[2])+\"AM.\"\n",
    "        \n",
    "        elif source == None or dest == None:\n",
    "            output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "    \n",
    "    elif intents == 'atis_airline':\n",
    "        if source!= None and dest!= None:\n",
    "            x = \"Here are the flights available\"\n",
    "            random_flights = random.sample(flights, 4)\n",
    "            output = x+\" from \"+source+\" to \"+dest+\": \"+random_flights[0]+\", \"+random_flights[1]+\", \"+random_flights[2]+\", \"+random_flights[3]+\".\"\n",
    "        \n",
    "        elif source == None or dest == None:\n",
    "            output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "    \n",
    "    elif intents == 'atis_airfare':\n",
    "        if source!= None and dest!= None:\n",
    "            x = \"These are the flights and price I could find\"\n",
    "            random_flights = random.sample(flights, 4)\n",
    "            random_fares = random.sample(flight_fares, 4)\n",
    "            output = x+\"from \"+source+\" to \"+dest+\": \"+random_flights[0]+\": \"+str(random_fares[0])+\" USD, \"+random_flights[1]+\": \"+str(random_fares[1])+\" USD, \"+random_flights[2]+\": \"+str(random_fares[2])+\" USD, \"+random_flights[3]+\": \"+str(random_fares[3])+\" USD.\"\n",
    "        \n",
    "        elif source == None or dest == None:\n",
    "            output = \"Sorry!! Will you please enter source and destination in the details?\"\n",
    "        \n",
    "    elif intents == 'atis_aircraft':\n",
    "        if source!= None and dest!= None:\n",
    "            x = \"Here are the list of types of flights used \"\n",
    "            random_types = random.sample(aircraft_types, 4)\n",
    "            output = x+\"from \"+source+\" to \"+dest+\": \"+random_types[0]+\": \"+random_types[0]+\", \"+random_types[1]+\", \"+random_types[2]+\", \"+random_types[3]+\".\"\n",
    "        \n",
    "        elif source == None or dest == None:\n",
    "            output = \"Sorry!! Can you please enter source and destination in the details?\"\n",
    "    \n",
    "\n",
    "    else:\n",
    "        output = \"Sorry, I didn't understand. I can only help you with Flight Details, Flight Fares, Flight timings and types of Flights. Please enter the correct information.\"\n",
    "    log.info(output)    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eb3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_simple('127.0.0.1', 8000, app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad79c4",
   "metadata": {},
   "source": [
    "#### CI/CD implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2e847",
   "metadata": {},
   "source": [
    "##### Continuous Integration using GIT\n",
    "\n",
    "Our github URL- https://github.com/DharaniRK/NLP_group (Private git repo, for only our team people are the collaborators)\n",
    "\n",
    "We have used github as our continuous integration tool since we are a team of 5 and each team member alters the code frequently. We have committed the initial version of code in default(master) branch from which we created individual branches based on the component/the person using a branch. \n",
    "\n",
    "Since github keeps the track of commit history, it was extremely useful when we need to rebase/revert the commit or merge the code among us\n",
    "\n",
    "We made use of the git feature- raising pull requests and merging the code to master branch regularly to avoid merge conflicts and any miss in the codebase. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c10c4b",
   "metadata": {},
   "source": [
    "##### Continuous deployment using Docker\n",
    "\n",
    "We used docker to deploy our web application since docker deployment is one of the most popular deployment technique for a microservice web application architecture. \n",
    "\n",
    "The base image we used in python:3.8-slim-buster which comes with linux operating system, python 3.8 and pip3 already installed.\n",
    "\n",
    "The docker image for our application is created with all the codebase moved to /app path in docker container and flask run command is executed\n",
    "\n",
    "Upon docker run, the container is created and the container acts as a chatbot. The container can be accessed using curl commands to localhost:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f18e4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!!! Nice meeting you! How can I help'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dialogue('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e660160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f5a0787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you for using ATIS chat bot! Have a good day'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dialogue('thanks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bb11040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from source\n",
      " whats the smallest plane that flies from pittsburgh to baltimore on eight sixteen\n",
      "SOURCE_LOC pittsburgh\n",
      "Loading from destination\n",
      " whats the smallest plane that flies from pittsburgh to baltimore on eight sixteen\n",
      "DESTINATION_LOC baltimore\n",
      "{'TIME': 'eight sixteen', 'source': 'pittsburgh', 'dest': 'baltimore'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Below are the flights from pittsburgh to baltimore :Alaska Airlines at 8PM, Virgin Airlines at 9PM, United Airlines at 6PM.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dialogue(' whats the smallest plane that flies from pittsburgh to baltimore on eight sixteen')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
