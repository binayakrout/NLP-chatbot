{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1e94f7",
   "metadata": {},
   "source": [
    "importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import important modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# sklearn modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB  # classifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# text preprocessing modules\n",
    "from string import punctuation\n",
    "# text preprocessing modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re  # regular expression\n",
    "import matplotlib.pyplot as plot\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.training import Example\n",
    "\n",
    "# text preprocessing modules\n",
    "from string import punctuation\n",
    "# text preprocessing modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re  # regular expression\n",
    "import os\n",
    "from os.path import dirname, join, realpath\n",
    "import joblib\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "nlppipeline = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner = nlppipeline.get_pipe(\"ner\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "optimizer = nlp.resume_training()\n",
    "\n",
    "# Download dependency\n",
    "for dependency in (\n",
    "        \"brown\",\n",
    "        \"names\",\n",
    "        \"wordnet\",\n",
    "        \"averaged_perceptron_tagger\",\n",
    "        \"universal_tagset\",\n",
    "):\n",
    "    nltk.download(dependency)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# seeding\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"ATIS data\",\n",
    "    description=\"A simple API that use NLP model to provide details about airline queries\",\n",
    "    version=\"0.1\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################Load data\n",
    "\n",
    "flight_details = pd.read_csv('atis_data.csv')\n",
    "\n",
    "df = pd.DataFrame(flight_details)\n",
    "\n",
    "\n",
    "print(flight_details.head())\n",
    "############ data visualization ###########\n",
    "def data_visualization(df):\n",
    "    fig, graph = plot.subplots()\n",
    "    p1 = graph.bar(df['Intents'], df['Input_Queries'], align='edge', width=0.3)\n",
    "    graph.yaxis.set_visible(False)\n",
    "    plot.show()\n",
    "    return df\n",
    "#######################data cleanup\n",
    "def data_preprocessing(df):\n",
    "    input_convos = df['Input_Queries']\n",
    "\n",
    "    # Removing punctuation\n",
    "    df['final_processed_data'] = input_convos.map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "    # Converting the dataset to lowercase\n",
    "    df['final_processed_data'] = input_convos.map(lambda x: x.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_processed_data = []\n",
    "    for data in df['final_processed_data']:\n",
    "        lemmatized_data = lemmatizer.lemmatize(data)\n",
    "        final_processed_data.append(lemmatized_data)\n",
    "\n",
    "    df['final_processed_data'] = final_processed_data\n",
    "    #df.head()\n",
    "    return df\n",
    "def data_train_test_split(df):\n",
    "    X=df['Input_Queries']\n",
    "    y=df['Intents']\n",
    "    X_train, X_test, y_train, y_test=train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "        stratify=y,\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def nlp_ner(df):\n",
    "    for data in df['final_processed_data'][0:5]:\n",
    "        query = nlp(data)\n",
    "        for word in query.ents:\n",
    "            print(word.text, word.label_)\n",
    "        displacy.render(query, style=\"ent\", jupyter=True)\n",
    "\n",
    "def rule_based_ner(df):\n",
    "    data = df['final_processed_data']\n",
    "    sourceloc = ''\n",
    "    destloc = ''\n",
    "    destination = []\n",
    "    source = []\n",
    "    source_data = []\n",
    "    destination_data = []\n",
    "    for doc in data:\n",
    "        words_list = doc.split()\n",
    "\n",
    "        if ' from ' in doc:\n",
    "            sourceloc = words_list[words_list.index('from') + 1]\n",
    "        elif ' leaving ' in doc:\n",
    "            sourceloc = words_list[words_list.index('leaving') + 1]\n",
    "        else:\n",
    "            doc = doc + ' na'\n",
    "            sourceloc = 'na'\n",
    "        nlpner = English()\n",
    "        ruler = nlpner.add_pipe(\"entity_ruler\")\n",
    "        source_rules = [{\"label\": \"source\", \"pattern\": [{\"LOWER\": sourceloc}]}]\n",
    "        sourceloc = ''\n",
    "        ruler.add_patterns(source_rules)\n",
    "\n",
    "        if ' in ' in doc:\n",
    "            destloc = words_list[words_list.index('in') + 1]\n",
    "        elif ' to ' in doc:\n",
    "            destloc = words_list[words_list.index('to') + 1]\n",
    "        else:\n",
    "            destloc = 'na'\n",
    "            doc = doc + ' na'\n",
    "        dest_rules = [{\"label\": \"destination\", \"pattern\": [{\"LOWER\": destloc}]}]\n",
    "        ruler.add_patterns(dest_rules)\n",
    "        destloc = ''\n",
    "        sourceloc = ''\n",
    "        doc1 = nlpner(doc)\n",
    "        for entity in doc1.ents:\n",
    "            if entity.label_ == 'source':\n",
    "                source.append(entity.text)\n",
    "                source_data.append((doc, {'entities': [(doc.index(entity.text),\n",
    "                                                        doc.index(entity.text) + len(entity.text),\n",
    "                                                        'SOURCE_LOC')]}))\n",
    "                break\n",
    "        for entity in doc1.ents:\n",
    "            if entity.label_ == 'destination':\n",
    "                destination.append(entity.text)\n",
    "                destination_data.append((doc, {'entities': [(doc.index(entity.text),\n",
    "                                                             doc.index(entity.text) + len(\n",
    "                                                                 entity.text),\n",
    "                                                             'DESTINATION_LOC')]}))\n",
    "                break\n",
    "\n",
    "        displacy.render(doc1, style=\"ent\", jupyter=True)\n",
    "\n",
    "    df['source'] = source\n",
    "    df['destination'] = destination\n",
    "    print(df.head())\n",
    "    return source_data,destination_data\n",
    "\n",
    "def ner_train_test_split(source_data,destination_data):\n",
    "    n = len(source_data)\n",
    "    print('Total data length: ', n)\n",
    "    train_data_size = n * 0.7\n",
    "    test_data_size = n * 0.3\n",
    "    source_train_data = source_data[0:int(train_data_size)]\n",
    "    source_test_data = source_data[int(train_data_size):]\n",
    "    destination_train_data = destination_data[0:int(train_data_size)]\n",
    "    destination_test_data = destination_data[int(train_data_size):]\n",
    "    print('source split: ', len(source_train_data), len(source_test_data))\n",
    "    print('destination split: ', len(destination_train_data), len(destination_test_data))\n",
    "    return source_train_data,source_test_data,destination_train_data,destination_test_data\n",
    "\n",
    "\n",
    "def custom_modelling(label, traindata):\n",
    "\n",
    "    output_path = ''\n",
    "    for _, annotates in traindata:\n",
    "        for ent in annotates.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    # Disabling the components otherthan the required ones\n",
    "    unaffected_pipelines = [pipeline for pipeline in nlppipeline.pipe_names if\n",
    "                            pipeline not in [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]]\n",
    "\n",
    "    # Model Training with 40 iterations so that it wont remember the data\n",
    "    with nlppipeline.disable_pipes(*unaffected_pipelines):\n",
    "\n",
    "        for iteration in range(30):\n",
    "            random.shuffle(traindata)\n",
    "            losses = {}\n",
    "            #  using spaCy's minibatch to batch up the train data\n",
    "            allbatches = minibatch(traindata, size=compounding(5.0, 30.0, 1.001))\n",
    "            for eachbatch in allbatches:\n",
    "                for txt, annotates in eachbatch:\n",
    "                    doc = nlppipeline.make_doc(txt)\n",
    "                    example = Example.from_dict(doc, annotates)\n",
    "                    # Running nlppipeline.update to adjust the weights\n",
    "                    nlppipeline.update([example], losses=losses, drop=0.3)\n",
    "                    # print(losses)\n",
    "\n",
    "    # Saving the model to path same as the label so that it can be loaded from the same path again\n",
    "\n",
    "    output_path = Path(label)\n",
    "    print(\"Saving the model to\", output_path)\n",
    "    nlppipeline.to_disk(output_path)\n",
    "\n",
    "\n",
    "def custom_spacy_ner(source_train_data,source_test_data,destination_train_data,destination_test_data):\n",
    "\n",
    "    custom_modelling('source', source_train_data)\n",
    "    custom_modelling('destination', destination_train_data)\n",
    "\n",
    "\n",
    "#data_preprocessing(df)\n",
    "#source_data, destination_data = rule_based_ner(df)\n",
    "#source_train_data, source_test_data, destination_train_data, destination_test_data = ner_train_test_split(    source_data, destination_data)\n",
    "#custom_spacy_ner(source_train_data, source_test_data, destination_train_data, destination_test_data)\n",
    "\n",
    "#data_visualization(df)\n",
    "\n",
    "@app.get(\"/get_ner\")\n",
    "def get_ner(query: str):\n",
    "    source = []\n",
    "    dest = []\n",
    "    for output_dir in ['source','destination']:\n",
    "        print(\"Loading from\", output_dir)\n",
    "        move_names = list(ner.move_names)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        #assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        print(query)\n",
    "        doc2 = nlp2(query)\n",
    "\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "            if ent.label_=='SOURCE_LOC':\n",
    "                obtained_source=ent.text\n",
    "                source.append(obtained_source)\n",
    "            if ent.label_=='DESTINATION_LOC':\n",
    "                obtained_dest=ent.text\n",
    "                dest.append(obtained_dest)\n",
    "\n",
    "    result={'source':[source],'dest':[dest]}\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
