{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743e9ef6",
   "metadata": {},
   "source": [
    "## Group Coursework-NATURAL LANGUAGE PROCESSING (COMM061) - Group 6\n",
    "\n",
    "#### Name: Binayak Rout, URN:6675500, Name: URN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd18539",
   "metadata": {},
   "source": [
    "###### About the dataset\n",
    "The dataset chosen contains data about Airline Travel. The dataset has the intents and user raw input conversations which contain the queries about the flights availability, flight fares, details about airlines, flight timings in it. \n",
    "\n",
    "https://www.kaggle.com/datasets/hassanamin/atis-airlinetravelinformationsystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7837e48",
   "metadata": {},
   "source": [
    "#### Web Service Model for the Chatbot\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb9f94",
   "metadata": {},
   "source": [
    "To host our NLP model for Intent classification, Name Entity recognition and dialogue flow manager, Flask API framework is choosen. Flask is python web development framework with an inbuilt web server, using which APIs are developed using different machine learning python libraries such as tensorflow, nltk, sklearn, keras, spacy etc. Flask is chosen as our serving model not only because it is easy to integrate with web pages and takes little effort to make a application up and running with API end points but also it is comparatively flexible and readable than anyother API deployment models. We did explore other  machine learning deployment model service such as FastAPI, Seldon Core, DeepDetect etc in which few of them focus only on serving one component and few are complex to understand and implement. FastAPI is known for its performance, but is relatively new, not many resources were available for achieving our goal. \n",
    "\n",
    "As our goal was to make a custom UI to serve our chatbot model, Flask was the right choice as it gives more flexibility to work with html pages and smooth dataflow between UI and backend. The application deployment is also easy and fast with Flask API. \n",
    "\n",
    "As we are going with docker deployment, base images for building a flask application(eg.python:3.8-slim-buster) are readily available in docker hub and installing the python libraries and making the application run as a container is quite easily acheviable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5ad095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intents</th>\n",
       "      <th>Input_Queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what's the arrival time in sanfrancisco for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Intents                                      Input_Queries\n",
       "0       atis_flight   i want to fly from boston at 838 am and arriv...\n",
       "1       atis_flight   what flights are available from pittsburgh to...\n",
       "2  atis_flight_time   what's the arrival time in sanfrancisco for t...\n",
       "3      atis_airfare            cheapest airfare from tacoma to orlando\n",
       "4      atis_airfare   round trip fares from pittsburgh to philadelp..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flight_details = pd.read_csv('atis_data.csv')\n",
    "\n",
    "df = pd.DataFrame(flight_details)\n",
    "\n",
    "\n",
    "flight_details.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2315bf46",
   "metadata": {},
   "source": [
    "#### Logging and monitoring\n",
    "We are using python logging library to track the events that happen in the application. Logging library helps to capture the log information with the level of severity - debug, info, warning, error and critical. \n",
    "\n",
    "Using format as current date time and display the log message beside the datetime so that the loggers are captured with the exact time when the event happened\n",
    "\n",
    "We are saving the log information into a file 'atis_details_bot.log' and maintaining in the same directory in which the code resides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4980ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='atis_ChatBot.log', encoding='utf-8', level=logging.DEBUG, format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d44d20-e312-4e27-828c-3ec1107eee8a",
   "metadata": {},
   "source": [
    "### Intent implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984402ae",
   "metadata": {},
   "source": [
    "   We have explored multiple models for intent classification in part 1 of the coursework. The experiments involved multiple train test split, hyper parameterization, pre-processing, testing, and fetching the correct intent for the given description. It was after evaluating multiple approaches, the team decided to narrow down the model to recurrent neural network.\n",
    "\n",
    "   The preprocessing technique is different for intents and sentences. The sentence is tokenized and padded with a size of 20. The intent is processed with a label encoder and one hot coding. The data was split into 70% for training and 30% for validation after the preprocessing was completed. The trained and validation split is used for model training. RNN with LSTM is used for training and the model is saved. \n",
    "               \n",
    "   It is after the chatbot fetches the intent, that the 'get intent' function is used where the input data is preprocessed as it was done earlier for the sentence. The saved model is loaded, and data is sent to the loaded model to predict the appropriate intent.\n",
    "               \n",
    "   The model has the capability to remember and train for length demanding time dependency.  Hence the collection of records can be used for modeling. Although training the model could be difficult, the intent classification of the model has given us an accuracy of 97.63. The f1 score is 0.971. RNN has been chosen because of its low false negatives and low false positives values. RNN performed better compared to MLP, perceptron, SVM, and CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0d799-eb51-4b71-b899-0e2a473657b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout, Dense, Activation, Flatten, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6202dc1-5ce9-44f4-85dc-dc9daf02c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load the dataset\n",
    "def intent_data_load():\n",
    "    \n",
    "    atis_trainData = pd.read_csv('atis_intents_train.csv')\n",
    "    atis_testData = pd.read_csv('atis_intents_test.csv')\n",
    "    col=atis_trainData.columns\n",
    "    df_length = len(atis_trainData)\n",
    "    atis_trainData.loc[df_length] = col\n",
    "    atis_trainData.columns =['Intent', 'Sentence']  #rename the column\n",
    "    col=atis_testData.columns\n",
    "    df_length = len(atis_testData)\n",
    "    atis_testData.loc[df_length] = col\n",
    "    atis_testData.columns =['Intent', 'Sentence']  #rename the column\n",
    "    combine = [atis_trainData, atis_testData]\n",
    "    flightData = pd.concat(combine)   #combine the dataset\n",
    "    intents=flightData.iloc[:,0]\n",
    "    intents=list(intents)             #intent column as list\n",
    "    sentence=flightData.iloc[:,1]\n",
    "    sentence=list(sentence)           #sentence column as list\n",
    "    \n",
    "    return intents,sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cf34e-348f-4939-88b1-1c757963947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for one hot coding\n",
    "def yonehot(intents):\n",
    "    laEn = LabelEncoder()\n",
    "    trans_intent = laEn.fit_transform(intents)              #convert intents into nos\n",
    "    trans_intent = laEn.transform(intents)\n",
    "    onehotintent = to_categorical(np.asarray(trans_intent)) #convert the nos into one hot coding\n",
    "    return onehotintent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defee844-1094-4429-bf53-1ad5a39ae29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split the dataset\n",
    "def datasplit(intents,sentence):\n",
    "    size = int(len(sentence)*0.3)                        #split the dataset into 70% train and 30% valiation\n",
    "    xtrain = sentence[size:] \n",
    "    ytrain = intents[size:] \n",
    "    xvalidation = sentence[:size] \n",
    "    yvalidation = intents[:size]\n",
    "    return xtrain,ytrain,xvalidation,yvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e083df4-c062-4640-a69a-20e0c9c16c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to preprocess the sentence\n",
    "def intent_preprocessing(inputstr):\n",
    "    intents,sentence = intent_data_load()\n",
    "    # text preprocessing\n",
    "    token = Tokenizer(num_words=5000)\n",
    "    token.fit_on_texts(sentence) \n",
    "    seq_inputstr = token.texts_to_sequences(inputstr)              #tokenize the sentences\n",
    "    pad_inputstr = sequence.pad_sequences(seq_inputstr, maxlen = 20)  #padding the tokenized sentences\n",
    "    return pad_inputstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9577f69-38ce-4aa1-993a-98735e65bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "def modeltrain(xtrain,ytrain,xvalidation,yvalidation):\n",
    "    \n",
    "    rnn_mod = Sequential()                                  #model initialization \n",
    "    rnn_mod.add(Embedding(5000, 60, input_length = 20)) \n",
    "    rnn_mod.add(LSTM(200))\n",
    "    rnn_mod.add(Dense(1000, activation='relu'))             #addition of layers\n",
    "    rnn_mod.add(Dense(600, activation='relu'))              #addition of layers\n",
    "    rnn_mod.add(Dense(8, activation='softmax'))             #addition of layers\n",
    "    rnn_mod.summary()\n",
    "    optimum = tf.keras.optimizers.Nadam(learning_rate=0.01, beta_1=0.8, beta_2=0.8,schedule_decay=0.002, epsilon=1e-08)     #parameter setting\n",
    "    rnn_mod.compile(loss='categorical_crossentropy', optimizer=optimum, metrics=['accuracy'])                               #model compile\n",
    "    rnn_mod.fit(xtrain,ytrain, epochs = 10, batch_size=50, verbose=1, validation_data=(xvalidation, yvalidation))           #model training\n",
    "    \n",
    "    return rnn_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4fe731-5f3f-45a9-bc6b-34b4bf01d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to train and store the model\n",
    "def trainandstore():\n",
    "    intents,sentence = intent_data_load()\n",
    "    intents = yonehot(intents)\n",
    "    xtrain,ytrain,xvalidation,yvalidation = datasplit(intents,sentence)\n",
    "    xtrain = intent_preprocessing(xtrain)\n",
    "    xvalidation = intent_preprocessing(xvalidation)\n",
    "    trainedmodel = modeltrain(xtrain,ytrain,xvalidation,yvalidation)\n",
    "    trainedmodel.save(\"trained_intent_model.h5\")          #store the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8117b-cbce-482c-babc-15966ba6b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to predict the user intent\n",
    "#@app.get(\"/get_intent\")\n",
    "def get_intent(inuptstr):\n",
    "    intent_model = keras.models.load_model(\"trained_intent_model.h5\")\n",
    "    processedinput = intent_preprocessing([inuptstr])\n",
    "    predictedintent = intent_model.predict(processedinput)\n",
    "    predictedintent=predictedintent.argmax(axis=-1)\n",
    "    label=['atis_abbreviation', 'atis_aircraft', 'atis_airfare', 'atis_airline', 'atis_flight', 'atis_flight_time', 'atis_ground_service', 'atis_quantity']\n",
    "    predictedintent_labeled=label[predictedintent[0]]\n",
    "    result={'userinput':inuptstr,'intent':predictedintent_labeled}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024c6c9",
   "metadata": {},
   "source": [
    "### NER implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a2e0a",
   "metadata": {},
   "source": [
    "To get the name entities for the given query, we initially used Spacy pretrained pipeline. It has some limitations while extracting few entities for this dataset,like GPE is specified for both the source and destination locations which doesn't give the exact information about whether the location is source or destination.Hence, we used Spacy rule based entity recognizer to extract the exact source and destination locations.\n",
    "\n",
    "Then the obtained entities are trained with Spacy custom NER with 70-30 train-test split and 50-50 train-test split and checked the metrics like accuracy, f1 score, recall and precision which was nearly 100% \n",
    "\n",
    "Using this approach, we retrived the both default entity labels using spacy pretrained pipeline, and source and destination entities from the given text. These entities can be combined to be used in the conversations to get the output for the given query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50784c26",
   "metadata": {},
   "source": [
    "#### Data preprocessing for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d019c8a",
   "metadata": {},
   "source": [
    "The data is already clean and doesnt contain nulls. Also, stop words removal cannot be done since the needed information for the rule based entity recognition('from','in','to' etc) will be removed\n",
    "\n",
    "Hence, applying basic preprocessing like removing the punctuations, making the words to lower case and performing Lemmatization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e72ad8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Dharani\n",
      "[nltk_data]     Rayadurgam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "nlppipeline = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner = nlppipeline.get_pipe(\"ner\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "optimizer = nlp.resume_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "983f0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    logging.info('Preprocessing for NER')\n",
    "    input_convos = df['Input_Queries']\n",
    "\n",
    "    # Removing punctuation\n",
    "    df['final_processed_data'] = input_convos.map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "    # Converting the dataset to lowercase\n",
    "    df['final_processed_data'] = input_convos.map(lambda x: x.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_processed_data = []\n",
    "    for data in df['final_processed_data']:\n",
    "        lemmatized_data = lemmatizer.lemmatize(data)\n",
    "        final_processed_data.append(lemmatized_data)\n",
    "\n",
    "    df['final_processed_data'] = final_processed_data\n",
    "    #df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb1563b",
   "metadata": {},
   "source": [
    "##### NER using spacy pipeline\n",
    "Getting the Name entities using the spacy pipeline 'en_core_web_sm' for entity labels other than GPE(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24b4ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_ner(df):\n",
    "    logging.info('Getting NER using spacy pipeline')\n",
    "    query = nlp(df)\n",
    "    ners = {}\n",
    "    for word in query.ents:\n",
    "        if word.label_ != 'GPE':\n",
    "            ners[word.label_] = word.text\n",
    "    logging.info('Obtained NERs from spacy pipeline', ners)\n",
    "    return ners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197ebc2",
   "metadata": {},
   "source": [
    "##### Model 1  Rule based entity recognizer using spacy\n",
    "Using these rules to distingish the source and destination location and adding them to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a67faf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rule_based_ner(df):\n",
    "    logging.debug('Entering NER rule based training ')\n",
    "    data = df['final_processed_data']\n",
    "    sourceloc = ''\n",
    "    destloc = ''\n",
    "    destination = []\n",
    "    source = []\n",
    "    source_data = []\n",
    "    destination_data = []\n",
    "    for doc in data:\n",
    "        words_list = doc.split()\n",
    "\n",
    "        if ' from ' in doc:\n",
    "            sourceloc = words_list[words_list.index('from') + 1]\n",
    "        elif ' leaving ' in doc:\n",
    "            sourceloc = words_list[words_list.index('leaving') + 1]\n",
    "        else:\n",
    "            doc = doc + ' na'\n",
    "            sourceloc = 'na'\n",
    "        nlpner = English()\n",
    "        ruler = nlpner.add_pipe(\"entity_ruler\")\n",
    "        source_rules = [{\"label\": \"source\", \"pattern\": [{\"LOWER\": sourceloc}]}]\n",
    "        sourceloc = ''\n",
    "        ruler.add_patterns(source_rules)\n",
    "\n",
    "        if ' in ' in doc:\n",
    "            destloc = words_list[words_list.index('in') + 1]\n",
    "        elif ' to ' in doc:\n",
    "            destloc = words_list[words_list.index('to') + 1]\n",
    "        else:\n",
    "            destloc = 'na'\n",
    "            doc = doc + ' na'\n",
    "        dest_rules = [{\"label\": \"destination\", \"pattern\": [{\"LOWER\": destloc}]}]\n",
    "        ruler.add_patterns(dest_rules)\n",
    "        destloc = ''\n",
    "        sourceloc = ''\n",
    "        doc1 = nlpner(doc)\n",
    "        for entity in doc1.ents:\n",
    "            if entity.label_ == 'source':\n",
    "                source.append(entity.text)\n",
    "                source_data.append((doc, {'entities': [(doc.index(entity.text),\n",
    "                                                        doc.index(entity.text) + len(entity.text),\n",
    "                                                        'SOURCE_LOC')]}))\n",
    "                break\n",
    "        for entity in doc1.ents:\n",
    "            if entity.label_ == 'destination':\n",
    "                destination.append(entity.text)\n",
    "                destination_data.append((doc, {'entities': [(doc.index(entity.text),\n",
    "                                                             doc.index(entity.text) + len(\n",
    "                                                                 entity.text),\n",
    "                                                             'DESTINATION_LOC')]}))\n",
    "                break\n",
    "\n",
    "\n",
    "    df['source'] = source\n",
    "    df['destination'] = destination\n",
    "    logging.debug('Completed NER rule based entity recognition')\n",
    "    return source_data,destination_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff14735",
   "metadata": {},
   "source": [
    "##### Inference from the above output\n",
    "By executing the above methods, the source and destination of the input query is properly distingused and provided with the rule defined labels - source and destination  and the data required for custom NER for source and destination is obtained\n",
    "\n",
    "Printing the dataset with the source and destination appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d107769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intents</th>\n",
       "      <th>Input_Queries</th>\n",
       "      <th>final_processed_data</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "      <td>boston</td>\n",
       "      <td>denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "      <td>pittsburgh</td>\n",
       "      <td>baltimore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what's the arrival time in sanfrancisco for t...</td>\n",
       "      <td>what's the arrival time in sanfrancisco for t...</td>\n",
       "      <td>washington</td>\n",
       "      <td>sanfrancisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "      <td>tacoma</td>\n",
       "      <td>orlando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "      <td>pittsburgh</td>\n",
       "      <td>philadelphia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Intents                                      Input_Queries  \\\n",
       "0       atis_flight   i want to fly from boston at 838 am and arriv...   \n",
       "1       atis_flight   what flights are available from pittsburgh to...   \n",
       "2  atis_flight_time   what's the arrival time in sanfrancisco for t...   \n",
       "3      atis_airfare            cheapest airfare from tacoma to orlando   \n",
       "4      atis_airfare   round trip fares from pittsburgh to philadelp...   \n",
       "\n",
       "                                final_processed_data      source   destination  \n",
       "0   i want to fly from boston at 838 am and arriv...      boston        denver  \n",
       "1   what flights are available from pittsburgh to...  pittsburgh     baltimore  \n",
       "2   what's the arrival time in sanfrancisco for t...  washington  sanfrancisco  \n",
       "3            cheapest airfare from tacoma to orlando      tacoma       orlando  \n",
       "4   round trip fares from pittsburgh to philadelp...  pittsburgh  philadelphia  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info('Performing NER preprocessing and rule based')\n",
    "data_preprocessing(df)\n",
    "source_data, destination_data = rule_based_ner(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8d602",
   "metadata": {},
   "source": [
    "##### Model 2 Custom NER modelling\n",
    "Using the labels derived above to train the data using spacy custom NER minibatch approach and fetching the loss for 30 iterations\n",
    "Stored the data as required to train with spacy custom NER in source_train_data with custom entity 'SOURCE_LOC' and destination_train_data with custom entity 'DESTINATION_LOC'\n",
    "Everytime, the data is shuffled and trained in batches for 30 iterations so that the model dont remember the labels as is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f47a965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlppipeline=spacy.load('en_core_web_sm')\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner=nlppipeline.get_pipe(\"ner\")\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0e5a550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ner_train_test_split(source_data,destination_data):\n",
    "    logging.info('Splitting the data to train and test')\n",
    "    n = len(source_data)\n",
    "    print('Total data length: ', n)\n",
    "    train_data_size = n * 0.7\n",
    "    test_data_size = n * 0.3\n",
    "    source_train_data = source_data[0:int(train_data_size)]\n",
    "    source_test_data = source_data[int(train_data_size):]\n",
    "    destination_train_data = destination_data[0:int(train_data_size)]\n",
    "    destination_test_data = destination_data[int(train_data_size):]\n",
    "    print('source split: ', len(source_train_data), len(source_test_data))\n",
    "    print('destination split: ', len(destination_train_data), len(destination_test_data))\n",
    "    return source_train_data,source_test_data,destination_train_data,destination_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c62aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_modelling(label, traindata):\n",
    "    logging.debug('Entering NER custom training model')\n",
    "\n",
    "    output_path = ''\n",
    "    for _, annotates in traindata:\n",
    "        for ent in annotates.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    # Disabling the components otherthan the required ones\n",
    "    unaffected_pipelines = [pipeline for pipeline in nlppipeline.pipe_names if\n",
    "                            pipeline not in [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]]\n",
    "\n",
    "    # Model Training with 40 iterations so that it wont remember the data\n",
    "    with nlppipeline.disable_pipes(*unaffected_pipelines):\n",
    "\n",
    "        for iteration in range(30):\n",
    "            random.shuffle(traindata)\n",
    "            losses = {}\n",
    "            #  using spaCy's minibatch to batch up the train data\n",
    "            allbatches = minibatch(traindata, size=compounding(5.0, 30.0, 1.001))\n",
    "            for eachbatch in allbatches:\n",
    "                for txt, annotates in eachbatch:\n",
    "                    doc = nlppipeline.make_doc(txt)\n",
    "                    example = Example.from_dict(doc, annotates)\n",
    "                    # Running nlppipeline.update to adjust the weights\n",
    "                    nlppipeline.update([example], losses=losses, drop=0.3)\n",
    "                    # print(losses)\n",
    "\n",
    "    # Saving the model to path same as the label so that it can be loaded from the same path again\n",
    "\n",
    "    output_path = Path(label)\n",
    "    logging.info(\"Saving the model to\", output_path)\n",
    "    nlppipeline.to_disk(output_path)\n",
    "    logging.debug('Leaving the NER custome model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cf25e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_spacy_ner(source_train_data,destination_train_data):\n",
    "    logging.info('Training NER custom model for source entity')\n",
    "    custom_modelling('source', source_train_data)\n",
    "    logging.info('Training NER custom model for destination entity')\n",
    "    custom_modelling('destination', destination_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a5a0728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data length:  199\n",
      "source split:  139 60\n",
      "destination split:  139 60\n"
     ]
    }
   ],
   "source": [
    "source_train_data, source_test_data, destination_train_data, destination_test_data = ner_train_test_split(source_data, destination_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "45826439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\logging\\__init__.py\", line 1083, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\logging\\__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\logging\\__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\logging\\__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\Dharani Rayadurgam\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/984242196.py\", line 1, in <module>\n",
      "    custom_spacy_ner(source_train_data, destination_train_data)\n",
      "  File \"C:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/3704397227.py\", line 3, in custom_spacy_ner\n",
      "    custom_modelling('source', source_train_data)\n",
      "  File \"C:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/2531400058.py\", line 31, in custom_modelling\n",
      "    logging.info(\"Saving the model to\", output_path)\n",
      "Message: 'Saving the model to'\n",
      "Arguments: (WindowsPath('source'),)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/984242196.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcustom_spacy_ner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_train_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/3704397227.py\u001b[0m in \u001b[0;36mcustom_spacy_ner\u001b[1;34m(source_train_data, destination_train_data)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcustom_modelling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'source'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training NER custom model for destination entity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mcustom_modelling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'destination'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\DHARAN~1\\AppData\\Local\\Temp/ipykernel_30920/2531400058.py\u001b[0m in \u001b[0;36mcustom_modelling\u001b[1;34m(label, traindata)\u001b[0m\n\u001b[0;32m     23\u001b[0m                     \u001b[0mexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                     \u001b[1;31m# Running nlppipeline.update to adjust the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                     \u001b[0mnlppipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                     \u001b[1;31m# print(losses)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;31m# ignore statements are used here because mypy ignores hasattr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"update\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msgd\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOutT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\ml\\tb_framework.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     step_model = ParserStepModel(\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\ml\\parser_model.pyx\u001b[0m in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\with_array.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSeqT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeqT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSeqT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         return _ragged_forward(\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRagged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRagged\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRagged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\with_array.py\u001b[0m in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     88\u001b[0m ) -> Tuple[Ragged, Callable]:\n\u001b[0;32m     89\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mArrayXd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mArrayXd\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_dX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataXd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdYr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRagged\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mRagged\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\concatenate.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\concatenate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\layers\\hashembed.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, ids, is_train)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mseed\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"seed\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0mdrop_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "custom_spacy_ner(source_train_data, destination_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa4fbc",
   "metadata": {},
   "source": [
    "#### NER inference\n",
    "Using Spacy NER pipeline, rule based NER recognistion and custom NER modelling, we were able to obtain the required name entities for the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aef1bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner(query):\n",
    "    logging.debug('Entering get_ner')\n",
    "    source = ''\n",
    "    dest = ''\n",
    "    all_entities=nlp_ner(query)\n",
    "    for output_dir in ['source','destination']:\n",
    "        logging.info(\"Loading from\", output_dir)\n",
    "        move_names = list(ner.move_names)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        #assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        logging.info(query)\n",
    "        doc2 = nlp2(query)\n",
    "\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "            if ent.label_=='SOURCE_LOC':\n",
    "                obtained_source=ent.text\n",
    "                source= obtained_source\n",
    "            if ent.label_=='DESTINATION_LOC':\n",
    "                obtained_dest=ent.text\n",
    "                dest=obtained_dest\n",
    "\n",
    "    location_entities={'source':source,'dest':dest}\n",
    "    result=all_entities | location_entities\n",
    "    logging.info(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630edde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11c07b72",
   "metadata": {},
   "source": [
    "### Using Heuristics for Dialogue flow Manager\n",
    "\n",
    "For Dialogue Flow, We used multiple models such as RNN(Recurrent Neural Network), LSTM(Long-Short Term Memory) and did encoding/decoding too for the dialogue flow management. For our dataset, we don't have responses for the questions being asked. Using models such as RNN,LSTM, was difficult to generate text from these models. And without proper pre-defined responses and enough data, it was difficult to train the RNN/LSTM models. \n",
    "\n",
    "Hence we decided to proceed with rule-based/Heuristics approach for Dialogue Manager. This method gets the intent and Entities for the given query, and using the heuristics, generates response easily. \n",
    "\n",
    "Here, since we dont have the output of the input queries in our dataset, we defined a format in the reply message based on the intents and entities and displaying the other values such as flight names, flight fares, air craft prices as a random value. \n",
    "\n",
    "Usually, the exact details of the flights names, timings, fares etc could be maintained in a database and using our intents and NERs, the exact value of each could be fetched using the queries and replaced in the random values that we display\n",
    "\n",
    "This mechanism does keep the history of previous conversations in application of each session/conversation. However, this model hasn't been utilisd fully to make use of all the history of the conversation. It can only predict on few parameters as of now. We could however make a mechanism to check if the user directs the question to some other topic other than what the bot is trained for. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc25e8b2",
   "metadata": {},
   "source": [
    "#### Including greetings and farewell in the dialogue flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2b4d7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from flask import Flask,request,jsonify\n",
    "from werkzeug.wrappers import Request, Response\n",
    "from werkzeug.serving import run_simple\n",
    "\n",
    "app = Flask(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662f64fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the flights from buffalo to oakland :JetBlue at 5PM, Alaska Airlines at 12PM, Breeze Airlines at 2PM.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "data = {\"userinput\": \"i need a flight tomorrow from buffalo to oakland\",\n",
    "    \"intent\": \"atis_flight\",\n",
    "        \"Entities\": {\"source\":\"buffalo\", \"dest\":\"oakland\"}}\n",
    "\n",
    "greetings= [\"hi\", \"hello\", \"hey\", \"helloo\", \"hellooo\", \"g morining\", \"gmorning\", \"good morning\",\n",
    "            \"morning\", \"good day\", \"good afternoon\", \"good evening\", \"greetings\",\n",
    "            \"greeting\", \"good to see you\", \"its good seeing you\", \"how are you\",\n",
    "            \"how're you\", \"how are you doing\", \"how ya doin'\", \"how ya doin\",\n",
    "             \"how is you\", \"how's you\", \"how is it going\", \"how's it going\", \"how's it goin'\",\n",
    "            \"how's it goin\", \"what is up\",  \"gday\", \"howdy\"]\n",
    "\n",
    "farewell=[\"Thank you\",\"Bye\", \"Good day\",\"good bye\",\"thanks\",\"have a good day\",\"cheers\",\"thank you so much for your help\",\"see you\",\n",
    "         \"bye-bye\", \"have a good day\"]\n",
    "\n",
    "\n",
    "def get_dialogue(data):\n",
    "\n",
    "    logger.info(\"Inside Dialogue Flow\")\n",
    "    \n",
    "    intents = data['intent']\n",
    "    entities = data['Entities']\n",
    "    output= None\n",
    "\n",
    "    input_query = data['userinput']\n",
    "\n",
    "    if input_query.lower() in greetings:\n",
    "        intents='greetings'\n",
    "        entities={}\n",
    "    elif input_query.lower() in farewell:\n",
    "        intents='farewell'\n",
    "        entities={}\n",
    "    else:\n",
    "        logger.info('Checking other intents by calling get_intents and fetching the NERs from get_ner method')\n",
    "        \n",
    "\n",
    "    logger.info(\"Intent is: \"+intents)\n",
    "    source = None\n",
    "    dest = None\n",
    "    time_dt = None\n",
    "\n",
    "    flights = ['Panam Airlines', 'American Airlines', 'Virgin Airlines', 'United Airlines', \n",
    "                'Breeze Airlines','Alaska Airlines','Frontier Airlines', 'JetBlue','Air India','British Airways','ABC Airlines']\n",
    "\n",
    "    aircraft_types = ['Airbus A320 family', 'Boeing 737 NG', 'Boeing 777','Airbus A330','Boeing 747','Airbus A319','Airbus A3303','Airbus 550M','Boeing 007']\n",
    "\n",
    "    flight_time = list(range(1,13))\n",
    "\n",
    "    flight_fares = list(range(100,1000,50))\n",
    "\n",
    "\n",
    "    if len(entities)!= 0:\n",
    "        if 'source' in entities.keys():\n",
    "            source = entities['source']\n",
    "        \n",
    "        if 'dest' in entities.keys():\n",
    "            dest = entities['dest']\n",
    "        \n",
    "        if 'TIME' in entities.keys():\n",
    "            time_dt = entities['TIME']\n",
    "        \n",
    "    \n",
    "    if intents == 'greetings':\n",
    "        output='Hello! I am Air Travel Information Bot. I can help you with flight details, airfares, type of flights.'\n",
    "\n",
    "    elif intents== 'farewell':\n",
    "        output='Thank you for using ATIS chat bot! Have a good day. Please click the Quit button to exit.'\n",
    "\n",
    "    elif intents == 'atis_flight':\n",
    "        if source!= None and dest!= None and source!='' and dest!='' and time_dt!= None:\n",
    "            random_flights = random.sample(flights, 3)\n",
    "            #random_time = random.sample(flight_time, 3)\n",
    "            output = \"Below are the flights from \"+source+\" to \"+dest+\" for \"+ time_dt + \" time :\"+random_flights[0]+\", \"+random_flights[1]+\", \"+random_flights[2]+\".\"\n",
    "        \n",
    "        elif source!= None and dest!= None and source!='' and dest!='':\n",
    "            random_flights = random.sample(flights, 3)\n",
    "            random_time = random.sample(flight_time, 3)\n",
    "            output = \"Below are the flights from \"+source+\" to \"+dest+\" :\"+random_flights[0]+\" at \"+str(random_time[0])+\"PM, \"+random_flights[1]+\" at \"+str(random_time[1])+\"PM, \"+random_flights[2]+\" at \"+str(random_time[2])+\"PM.\"\n",
    "        \n",
    "        elif source == None or source == '':\n",
    "            if dest!=None and dest!='':\n",
    "                output = \"Please enter the source\"\n",
    "            elif dest == None or dest == '':\n",
    "                output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "        \n",
    "        elif dest == None or dest == '':\n",
    "            if source!=None and source!='':\n",
    "                output = \"Please enter the destination\"\n",
    "            elif dest == None or dest == '':\n",
    "                output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "      \n",
    "\n",
    "    elif intents == 'atis_flight_time':\n",
    "        if source!= None and dest!= None and source!='' and dest!='' and time_dt!= None:\n",
    "            random_flights = random.sample(flights, 3)\n",
    "            #random_time = random.sample(flight_time, 3)\n",
    "            output = \"Below are the flights from \"+source+\" to \"+dest+\" for \"+ time_dt + \" time :\"+random_flights[0]+\", \"+random_flights[1]+\", \"+random_flights[2]+\".\"\n",
    "        \n",
    "        elif source!= None and dest!= None and source!='' and dest!='':\n",
    "            x = \"Here are the timings and flights available for today \"\n",
    "            random_flights = random.sample(flights, 3)\n",
    "            random_time = random.sample(flight_time, 3)\n",
    "            random_time.sort()\n",
    "            output = x+\"from \"+source+\" to \"+dest+\": \"+random_flights[0]+\" at \"+str(random_time[0])+\"PM, \"+random_flights[1]+\" at \"+str(random_time[1])+\"PM, \"+random_flights[2]+\" at \"+str(random_time[2])+\"AM.\"\n",
    "        \n",
    "        elif source == None or source == '':\n",
    "            if dest!=None and dest!='':\n",
    "                output = \"Please enter the source\"\n",
    "            elif dest == None or dest == '':\n",
    "                output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "        \n",
    "        elif dest == None or dest == '':\n",
    "            if source!=None and source!='':\n",
    "                output = \"Please enter the destination\"\n",
    "            elif dest == None or dest == '':\n",
    "                output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "        \n",
    "    \n",
    "    elif intents == 'atis_airline':\n",
    "        if source!= None and dest!= None and source!='' and dest!='' and time_dt!= None:\n",
    "            random_flights = random.sample(flights, 3)\n",
    "            #random_time = random.sample(flight_time, 3)\n",
    "            output = \"Below are the flights from \"+source+\" to \"+dest+\" for \"+ time_dt + \" time :\"+random_flights[0]+\", \"+random_flights[1]+\", \"+random_flights[2]+\".\"\n",
    "        \n",
    "        elif source!= None and dest!= None and source!='' and dest!='':\n",
    "            x = \"Here are the flights available\"\n",
    "            random_flights = random.sample(flights, 4)\n",
    "            output = x+\" from \"+source+\" to \"+dest+\": \"+random_flights[0]+\", \"+random_flights[1]+\", \"+random_flights[2]+\", \"+random_flights[3]+\".\"\n",
    "        \n",
    "        elif source == None or source == '':\n",
    "            if dest!=None and dest!='':\n",
    "                output = \"Please enter the source\"\n",
    "            elif dest == None or dest == '':\n",
    "                output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "        \n",
    "        elif dest == None or dest == '':\n",
    "            if source!=None and source!='':\n",
    "                output = \"Please enter the destination\"\n",
    "            elif dest == None or dest == '':\n",
    "                output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "        \n",
    "    \n",
    "    elif intents == 'atis_airfare':\n",
    "        if source!= None and dest!= None and source!='' and dest!='' and time_dt!= None:\n",
    "            x = \"These are the flights and price I could find\"\n",
    "            random_flights = random.sample(flights, 4)\n",
    "            random_fares = random.sample(flight_fares, 4)\n",
    "            #random_time = random.sample(flight_time, 3)\n",
    "            output = x+\"from \"+source+\" to \"+dest+\" for \"+ time_dt + \" time :\"+random_flights[0]+\": \"+str(random_fares[0])+\" USD, \"+random_flights[1]+\": \"+str(random_fares[1])+\" USD, \"+random_flights[2]+\": \"+str(random_fares[2])+\" USD, \"+random_flights[3]+\": \"+str(random_fares[3])+\" USD.\"\n",
    "        \n",
    "        elif source!= None and dest!= None and source!='' and dest!='':\n",
    "            x = \"These are the flights and price I could find\"\n",
    "            random_flights = random.sample(flights, 4)\n",
    "            random_fares = random.sample(flight_fares, 4)\n",
    "            output = x+\"from \"+source+\" to \"+dest+\": \"+random_flights[0]+\": \"+str(random_fares[0])+\" USD, \"+random_flights[1]+\": \"+str(random_fares[1])+\" USD, \"+random_flights[2]+\": \"+str(random_fares[2])+\" USD, \"+random_flights[3]+\": \"+str(random_fares[3])+\" USD.\"\n",
    "        \n",
    "        elif source == None or source == '':\n",
    "            if dest!=None and dest!='':\n",
    "                output = \"Please enter the source\"\n",
    "            elif dest == None or dest == '':\n",
    "                output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "        \n",
    "        elif dest == None or dest == '':\n",
    "            if source!=None and source!='':\n",
    "                output = \"Please enter the destination\"\n",
    "            elif dest == None or dest == '':\n",
    "                output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "        \n",
    "        \n",
    "    elif intents == 'atis_aircraft':\n",
    "        if source!= None and dest!= None and source!='' and dest!='' and time_dt!= None:\n",
    "            x = \"Here are the list of types of flights used \"\n",
    "            random_flights = random.sample(aircraft_types, 3)\n",
    "            #random_time = random.sample(flight_time, 3)\n",
    "            output =  x+\"from \"+source+\" to \"+dest+\" for \"+ time_dt + \" time :\"+random_flights[0]+\", \"+random_flights[1]+\", \"+random_flights[2]+\".\"\n",
    "        \n",
    "        elif source!= None and dest!= None and source!='' and dest!='':\n",
    "            x = \"Here are the list of types of flights used \"\n",
    "            random_types = random.sample(aircraft_types, 4)\n",
    "            output = x+\"from \"+source+\" to \"+dest+\": \"+random_types[0]+\": \"+random_types[0]+\", \"+random_types[1]+\", \"+random_types[2]+\", \"+random_types[3]+\".\"\n",
    "        \n",
    "        elif source == None or source == '':\n",
    "            if dest!=None and dest!='':\n",
    "                output = \"Please enter the source\"\n",
    "            elif dest == None or dest == '':\n",
    "                output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "        \n",
    "        elif dest == None or dest == '':\n",
    "            if source!=None and source!='':\n",
    "                output = \"Please enter the destination\"\n",
    "            elif dest == None or dest == '':\n",
    "                output = \"Sorry!! Please enter the source and destination in your details\"\n",
    "        \n",
    "    \n",
    "\n",
    "    else:\n",
    "        output = \"Sorry, I didn't understand. I can only help you with Flight Details, Flight Fares, Flight timings and types of Flights. Please enter the correct information.\"\n",
    "        \n",
    "\n",
    "\n",
    "    logger.info(\"End of Dialogue Flow\")\n",
    "        \n",
    "\n",
    "#     print(output)\n",
    "    return output\n",
    "    \n",
    "\n",
    "response = get_dialogue(data);\n",
    "print(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eb3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_simple('127.0.0.1', 8080, app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c368bb6",
   "metadata": {},
   "source": [
    "### Functionality Testing of chatbot\n",
    "\n",
    "Our chatbot application is built with Flask and http endpoints('/chat' and '/quit'). '/chat' endpoint gets the response from chatbot using intents, Named-entity extraction on the input text of user. We have created UI using html/Jquery to access these endpoints and provide an interactive chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e2103",
   "metadata": {},
   "source": [
    " <img src=\"successful_snap.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff650bf",
   "metadata": {},
   "source": [
    "Above image is a snapshot from our chatbot application. Here we are successfully making requests to chatbot for which the chatbot is correctly responding </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa02c6",
   "metadata": {},
   "source": [
    " <img src=\"different_intents.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0644cc7",
   "metadata": {},
   "source": [
    "Here, we have tested our chatbot with different intents and started the chatbot with 'hi' and ended the converstation with 'Thanks'. The chatbot gave proper results as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98521ca",
   "metadata": {},
   "source": [
    " <img src=\"different_request_override.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a24e6",
   "metadata": {},
   "source": [
    "In the above testing, it has been shown that the chatbot utilises the previous history and gave the proper output such as utilising the history of conversation and getting the source/destination from previous conversation to make a desired response. We have also overridden the previous source and destination with the new ones.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768e34a",
   "metadata": {},
   "source": [
    " <img src=\"confusion.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec201f",
   "metadata": {},
   "source": [
    "In this test, we tried to confuse the chatbot by adding 'hi' in the middle of conversation. The chatbot didn't respond well and provided different output than expected because of use of history for getting source data. It was expected as our history is not yet fully utilised to cover all the scenarios to make it an intelligent bot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ef182",
   "metadata": {},
   "source": [
    " <img src=\"after_quit.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b88e1",
   "metadata": {},
   "source": [
    "In the end, we are able to get to 'quit' endpoint to end our conversation chatbot through quit button on home page. We can then access the chatbot through the button as shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e3614",
   "metadata": {},
   "source": [
    "### Performance of Chatbot\n",
    "\n",
    "Chatbot performed very well handling multiple scenarios. It was able to correctly predict intents and extract desired entities most of the cases. With the history mechanism, chatbot was able to utilise previous conversation memory to make a response based on new ones. However, it was not able to handle properly when the user starts the new conversation in between the current conversation which confused the bot. It can be improved with better history/past conversation management like using AWS S3 for storage of the history . The chatbot did take around 1 second to respond. It can be improved through less logging and more tools to improve the performance of the application as a whole. The chatbot will be easily scaled and trained with much larger dataset and more features by making them as microservice and communicate each component through docker. The components are already in different python files which can be scaled easily. The chatbot was tested with multiple conversation and it didn't break."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c86651",
   "metadata": {},
   "source": [
    "### Monitoring capability\n",
    "\n",
    "For Monitoring, we have utilised \"Logging\" which is used for python. A single log is being generated throughout the application named \"atis_details_bot.log\" which logs each component. It logs the predicted intent and also the entity extracted from the user input and also the response generated through the dialogue flow manager. With this, we were able to monitor our chatbot performance and the health of the chatbot. Below is the snapshot of our log.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60662dc",
   "metadata": {},
   "source": [
    " <img src=\"logging.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad79c4",
   "metadata": {},
   "source": [
    "#### CI/CD implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2e847",
   "metadata": {},
   "source": [
    "##### Continuous Integration using GIT\n",
    "\n",
    "Our github URL- https://github.com/DharaniRK/NLP_group (Private git repo, for only our team people are the collaborators)\n",
    "\n",
    "We have used github as our continuous integration tool since we are a team of 5 and each team member alters the code frequently. We have committed the initial version of code in default(master) branch from which we created individual branches based on the component/the person using a branch. \n",
    "\n",
    "Since github keeps the track of commit history, it was extremely useful when we need to rebase/revert the commit or merge the code among us\n",
    "\n",
    "We made use of the git feature- raising pull requests and merging the code to master branch regularly to avoid merge conflicts and any miss in the codebase. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2c186a1",
   "metadata": {},
   "source": [
    " <img src=\"git.PNG\" height = 300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c10c4b",
   "metadata": {},
   "source": [
    "##### Continuous deployment using Docker\n",
    "\n",
    "We used docker to deploy our web application since docker deployment is one of the most popular deployment technique for a microservice web application architecture. \n",
    "\n",
    "The base image we used in python:3.8-slim-buster which comes with linux operating system, python 3.8 and pip3 already installed.\n",
    "\n",
    "The docker image for our application is created with all the codebase moved to /app path in docker container and flask run command is executed\n",
    "\n",
    "Upon docker run, the container is created and the container acts as a chatbot. The container can be accessed using curl commands to localhost:8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f18e4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!!! Nice meeting you! How can I help'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dialogue('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e660160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f5a0787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you for using ATIS chat bot! Have a good day'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dialogue('thanks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bb11040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from source\n",
      " whats the smallest plane that flies from pittsburgh to baltimore on eight sixteen\n",
      "SOURCE_LOC pittsburgh\n",
      "Loading from destination\n",
      " whats the smallest plane that flies from pittsburgh to baltimore on eight sixteen\n",
      "DESTINATION_LOC baltimore\n",
      "{'TIME': 'eight sixteen', 'source': 'pittsburgh', 'dest': 'baltimore'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Below are the flights from pittsburgh to baltimore :Alaska Airlines at 8PM, Virgin Airlines at 9PM, United Airlines at 6PM.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dialogue(' whats the smallest plane that flies from pittsburgh to baltimore on eight sixteen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635fd7b",
   "metadata": {},
   "source": [
    "Please find the working code for chatbot application in the zipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc55b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
